{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Territorial.io Vision Model Training\n",
    "## Train a CNN to classify game map regions\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload this notebook to [Kaggle](https://kaggle.com/notebooks)\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2 (free)\n",
    "3. Run all cells\n",
    "4. Download `vision_model.pth` from the Output tab\n",
    "5. Place it in `territorial_bot/models/vision_model.pth`\n",
    "\n",
    "This notebook:\n",
    "- Generates synthetic training data from Territorial.io color patterns\n",
    "- Trains a CNN (TerritoryClassifierCNN) to classify map patches\n",
    "- Exports the trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision opencv-python-headless Pillow numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle output directory\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Data Generation\n",
    "\n",
    "Since we can't scrape the live game directly in Kaggle, we generate realistic synthetic training data\n",
    "based on Territorial.io's known color palette and visual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# TERRITORIAL.IO COLOR PALETTE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# Territory colors (RGB) - these are the actual colors used in the game\n",
    "TERRITORY_COLORS = {\n",
    "    'own': [\n",
    "        (0, 100, 255),    # Blue (default player color)\n",
    "        (0, 150, 255),\n",
    "        (30, 120, 220),\n",
    "        (0, 80, 200),\n",
    "    ],\n",
    "    'enemy': [\n",
    "        (255, 50, 50),    # Red\n",
    "        (220, 30, 30),\n",
    "        (255, 80, 80),\n",
    "        (200, 0, 0),\n",
    "        (255, 140, 0),    # Orange enemy\n",
    "        (180, 0, 255),    # Purple enemy\n",
    "        (50, 200, 50),    # Green enemy\n",
    "        (255, 220, 0),    # Yellow enemy\n",
    "        (0, 220, 220),    # Cyan enemy\n",
    "        (255, 100, 180),  # Pink enemy\n",
    "    ],\n",
    "    'neutral': [\n",
    "        (180, 180, 180),  # Light gray\n",
    "        (160, 160, 160),\n",
    "        (200, 200, 200),\n",
    "        (140, 140, 140),\n",
    "        (170, 170, 170),\n",
    "    ],\n",
    "    'border': [\n",
    "        (10, 10, 10),     # Near black\n",
    "        (20, 20, 20),\n",
    "        (5, 5, 5),\n",
    "        (30, 30, 30),\n",
    "    ],\n",
    "}\n",
    "\n",
    "CLASSES = ['own', 'enemy', 'neutral', 'border']\n",
    "PATCH_SIZE = 64  # CNN input size\n",
    "SAMPLES_PER_CLASS = 2000\n",
    "\n",
    "print(f'Classes: {CLASSES}')\n",
    "print(f'Patch size: {PATCH_SIZE}x{PATCH_SIZE}')\n",
    "print(f'Samples per class: {SAMPLES_PER_CLASS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_territory_patch(label: str, size: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a realistic synthetic patch for a given territory class.\n",
    "    Adds noise, gradients, and texture to simulate real game visuals.\n",
    "    \"\"\"\n",
    "    colors = TERRITORY_COLORS[label]\n",
    "    base_color = random.choice(colors)\n",
    "    \n",
    "    # Create base patch\n",
    "    patch = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "    \n",
    "    if label == 'border':\n",
    "        # Dark with slight variation\n",
    "        noise = np.random.randint(0, 15, (size, size, 3), dtype=np.uint8)\n",
    "        patch[:] = base_color\n",
    "        patch = np.clip(patch.astype(int) + noise - 7, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Add border lines\n",
    "        if random.random() > 0.5:\n",
    "            cv2.line(patch, (0, size//2), (size, size//2), (50, 50, 50), 2)\n",
    "        if random.random() > 0.5:\n",
    "            cv2.line(patch, (size//2, 0), (size//2, size), (50, 50, 50), 2)\n",
    "    \n",
    "    elif label == 'neutral':\n",
    "        # Uniform gray with texture\n",
    "        patch[:] = base_color\n",
    "        noise = np.random.randint(-20, 20, (size, size, 3))\n",
    "        patch = np.clip(patch.astype(int) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Add subtle grid pattern\n",
    "        if random.random() > 0.7:\n",
    "            for i in range(0, size, 8):\n",
    "                patch[i, :] = np.clip(patch[i, :].astype(int) - 10, 0, 255)\n",
    "                patch[:, i] = np.clip(patch[:, i].astype(int) - 10, 0, 255)\n",
    "    \n",
    "    else:  # own or enemy\n",
    "        # Solid color with gradient and noise\n",
    "        patch[:] = base_color\n",
    "        \n",
    "        # Add gradient\n",
    "        gradient = np.linspace(0, 30, size).astype(int)\n",
    "        for i in range(size):\n",
    "            patch[i] = np.clip(\n",
    "                patch[i].astype(int) + gradient[i] - 15, 0, 255\n",
    "            ).astype(np.uint8)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.randint(-15, 15, (size, size, 3))\n",
    "        patch = np.clip(patch.astype(int) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Occasionally add troop number text simulation\n",
    "        if random.random() > 0.6:\n",
    "            num = str(random.randint(1, 999))\n",
    "            cv2.putText(\n",
    "                patch, num,\n",
    "                (random.randint(5, 20), random.randint(20, 50)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.4, (255, 255, 255), 1\n",
    "            )\n",
    "        \n",
    "        # Add border edge effect\n",
    "        if random.random() > 0.5:\n",
    "            edge_width = random.randint(1, 3)\n",
    "            patch[:edge_width, :] = np.clip(\n",
    "                patch[:edge_width, :].astype(int) - 40, 0, 255\n",
    "            ).astype(np.uint8)\n",
    "    \n",
    "    return patch\n",
    "\n",
    "\n",
    "def generate_dataset(samples_per_class: int = 2000):\n",
    "    \"\"\"Generate the full synthetic training dataset.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in CLASSES:\n",
    "        print(f'Generating {samples_per_class} samples for class: {label}')\n",
    "        for _ in range(samples_per_class):\n",
    "            patch = generate_territory_patch(label, PATCH_SIZE)\n",
    "            images.append(patch)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "print('Generating synthetic training data...')\n",
    "images, labels = generate_dataset(SAMPLES_PER_CLASS)\n",
    "print(f'Dataset: {images.shape}, Labels: {labels.shape}')\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    cls_indices = np.where(labels == cls)[0]\n",
    "    for j in range(8):\n",
    "        idx = cls_indices[j]\n",
    "        axes[i, j].imshow(cv2.cvtColor(images[idx], cv2.COLOR_BGR2RGB))\n",
    "        axes[i, j].set_title(cls, fontsize=8)\n",
    "        axes[i, j].axis('off')\n",
    "plt.suptitle('Synthetic Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/training_samples.png', dpi=100)\n",
    "plt.show()\n",
    "print('Sample visualization saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(CLASSES)\n",
    "label_ids = le.transform(labels)\n",
    "print(f'Label mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}')\n",
    "\n",
    "# Save label encoder\n",
    "with open(f'{OUTPUT_DIR}/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "print('Label encoder saved.')\n",
    "\n",
    "\n",
    "class TerritoryDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for territory patch classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None, augment=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].copy()\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            img = self._augment(img)\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        \n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(pil_img)\n",
    "        else:\n",
    "            img_tensor = transforms.ToTensor()(pil_img)\n",
    "        \n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def _augment(self, img):\n",
    "        \"\"\"Apply random augmentations.\"\"\"\n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "        \n",
    "        # Random vertical flip\n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 0)\n",
    "        \n",
    "        # Random rotation (0, 90, 180, 270)\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            img = np.rot90(img, k).copy()\n",
    "        \n",
    "        # Random brightness\n",
    "        if random.random() > 0.5:\n",
    "            factor = random.uniform(0.7, 1.3)\n",
    "            img = np.clip(img.astype(float) * factor, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Random Gaussian noise\n",
    "        if random.random() > 0.5:\n",
    "            noise = np.random.normal(0, 10, img.shape).astype(np.int16)\n",
    "            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    images, label_ids, test_size=0.2, random_state=42, stratify=label_ids\n",
    ")\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}')\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and loaders\n",
    "train_dataset = TerritoryDataset(X_train, y_train, transform=train_transform, augment=True)\n",
    "val_dataset = TerritoryDataset(X_val, y_val, transform=val_transform, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerritoryClassifierCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight CNN for classifying game map patches.\n",
    "    Input: 64x64 RGB patch\n",
    "    Output: 4-class softmax (own, enemy, neutral, border)\n",
    "    \n",
    "    Architecture matches vision_system.py for direct weight loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 4):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 32x32\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 16x16\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = TerritoryClassifierCNN(num_classes=len(CLASSES)).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_path = f'{OUTPUT_DIR}/vision_model_best.pth'\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100.0 * correct / total\n",
    "\n",
    "\n",
    "def val_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100.0 * correct / total\n",
    "\n",
    "\n",
    "print(f'Starting training for {EPOCHS} epochs...')\n",
    "print('=' * 60)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = val_epoch(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        marker = ' ← BEST'\n",
    "    else:\n",
    "        marker = ''\n",
    "    \n",
    "    print(\n",
    "        f'Epoch {epoch:3d}/{EPOCHS} | '\n",
    "        f'Train Loss: {train_loss:.4f} Acc: {train_acc:.1f}% | '\n",
    "        f'Val Loss: {val_loss:.4f} Acc: {val_acc:.1f}%{marker}'\n",
    "    )\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'Training complete! Best val accuracy: {best_val_acc:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss')\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(epochs_range, history['train_acc'], 'b-', label='Train Acc')\n",
    "ax2.plot(epochs_range, history['val_acc'], 'r-', label='Val Acc')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/training_curves.png', dpi=100)\n",
    "plt.show()\n",
    "print('Training curves saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.numpy())\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(\n",
    "    all_targets, all_preds,\n",
    "    target_names=le.classes_\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=le.classes_, yticklabels=le.classes_\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/confusion_matrix.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model (same as best)\n",
    "final_model_path = f'{OUTPUT_DIR}/vision_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f'Final model saved to: {final_model_path}')\n",
    "\n",
    "# Verify model can be loaded\n",
    "test_model = TerritoryClassifierCNN(num_classes=len(CLASSES))\n",
    "test_model.load_state_dict(torch.load(final_model_path, map_location='cpu'))\n",
    "test_model.eval()\n",
    "\n",
    "# Test inference\n",
    "dummy_input = torch.randn(1, 3, 64, 64)\n",
    "with torch.no_grad():\n",
    "    output = test_model(dummy_input)\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "print(f'Test inference output shape: {output.shape}')\n",
    "print(f'Test probabilities: {probs.numpy()}')\n",
    "print('Model verification passed!')\n",
    "\n",
    "# List all output files\n",
    "print('\\nOutput files:')\n",
    "for f in sorted(Path(OUTPUT_DIR).iterdir()):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f'  {f.name}: {size_kb:.1f} KB')\n",
    "\n",
    "print('\\n✅ DONE! Download vision_model.pth and label_encoder.pkl')\n",
    "print('   Place them in territorial_bot/models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
