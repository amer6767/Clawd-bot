{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Territorial.io Brain Model Training (DQN)\n",
    "## Train a Deep Q-Network for strategic decision making\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload this notebook to [Kaggle](https://kaggle.com/notebooks)\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2 (free)\n",
    "3. Run all cells\n",
    "4. Download `brain_model.pth` from the Output tab\n",
    "5. Place it in `territorial_bot/models/brain_model.pth`\n",
    "\n",
    "This notebook:\n",
    "- Builds a simulated Territorial.io game environment\n",
    "- Trains a DQN agent using reinforcement learning\n",
    "- Exports the trained model weights for use in the live bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulated Game Environment\n",
    "\n",
    "We simulate a simplified Territorial.io grid world for RL training.\n",
    "The agent controls territory expansion on a 20x20 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerritorialEnv:\n",
    "    \"\"\"\n",
    "    Simplified Territorial.io simulation environment.\n",
    "    \n",
    "    Grid cells can be:\n",
    "      0 = neutral\n",
    "      1 = own territory\n",
    "     -1 = enemy territory\n",
    "    \n",
    "    Actions: 0=STAY, 1=N, 2=NE, 3=E, 4=SE, 5=S, 6=SW, 7=W, 8=NW\n",
    "    \"\"\"\n",
    "    \n",
    "    GRID_SIZE = 20\n",
    "    NUM_ENEMIES = 3\n",
    "    \n",
    "    # Direction vectors (row, col)\n",
    "    DIRECTIONS = [\n",
    "        (0, 0),   # STAY\n",
    "        (-1, 0),  # N\n",
    "        (-1, 1),  # NE\n",
    "        (0, 1),   # E\n",
    "        (1, 1),   # SE\n",
    "        (1, 0),   # S\n",
    "        (1, -1),  # SW\n",
    "        (0, -1),  # W\n",
    "        (-1, -1), # NW\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grid = None\n",
    "        self.own_cells = set()\n",
    "        self.enemy_cells = [set() for _ in range(self.NUM_ENEMIES)]\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 500\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.grid = np.zeros((self.GRID_SIZE, self.GRID_SIZE), dtype=np.float32)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Place own territory (3x3 block in random corner)\n",
    "        corners = [\n",
    "            (1, 1), (1, self.GRID_SIZE-4),\n",
    "            (self.GRID_SIZE-4, 1), (self.GRID_SIZE-4, self.GRID_SIZE-4)\n",
    "        ]\n",
    "        own_corner = random.choice(corners)\n",
    "        self.own_cells = set()\n",
    "        for dr in range(3):\n",
    "            for dc in range(3):\n",
    "                r, c = own_corner[0] + dr, own_corner[1] + dc\n",
    "                self.grid[r, c] = 1.0\n",
    "                self.own_cells.add((r, c))\n",
    "        \n",
    "        # Place enemies (2x2 blocks in other corners)\n",
    "        remaining_corners = [c for c in corners if c != own_corner]\n",
    "        self.enemy_cells = [set() for _ in range(self.NUM_ENEMIES)]\n",
    "        for i, corner in enumerate(remaining_corners[:self.NUM_ENEMIES]):\n",
    "            for dr in range(2):\n",
    "                for dc in range(2):\n",
    "                    r, c = corner[0] + dr, corner[1] + dc\n",
    "                    self.grid[r, c] = -1.0\n",
    "                    self.enemy_cells[i].add((r, c))\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"Execute action and return (next_state, reward, done, info).\"\"\"\n",
    "        self.step_count += 1\n",
    "        prev_own = len(self.own_cells)\n",
    "        prev_enemy_total = sum(len(e) for e in self.enemy_cells)\n",
    "        \n",
    "        # Execute own action\n",
    "        if action != 0:  # Not STAY\n",
    "            self._expand_territory(action)\n",
    "        \n",
    "        # Execute enemy actions (simple AI)\n",
    "        for i in range(self.NUM_ENEMIES):\n",
    "            if self.enemy_cells[i]:\n",
    "                self._enemy_expand(i)\n",
    "        \n",
    "        # Compute reward\n",
    "        curr_own = len(self.own_cells)\n",
    "        curr_enemy_total = sum(len(e) for e in self.enemy_cells)\n",
    "        \n",
    "        territory_gain = (curr_own - prev_own) * 0.5\n",
    "        enemy_loss = (prev_enemy_total - curr_enemy_total) * 0.3\n",
    "        time_penalty = -0.01\n",
    "        \n",
    "        reward = territory_gain + enemy_loss + time_penalty\n",
    "        \n",
    "        # Check done conditions\n",
    "        total_cells = self.GRID_SIZE * self.GRID_SIZE\n",
    "        own_pct = curr_own / total_cells\n",
    "        \n",
    "        done = False\n",
    "        if curr_own == 0:\n",
    "            reward = -10.0  # Died\n",
    "            done = True\n",
    "        elif own_pct > 0.6:\n",
    "            reward = +20.0  # Dominant victory\n",
    "            done = True\n",
    "        elif all(len(e) == 0 for e in self.enemy_cells):\n",
    "            reward = +15.0  # Eliminated all enemies\n",
    "            done = True\n",
    "        elif self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "        \n",
    "        info = {\n",
    "            'own_pct': own_pct,\n",
    "            'own_cells': curr_own,\n",
    "            'enemy_cells': curr_enemy_total,\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _expand_territory(self, action: int):\n",
    "        \"\"\"Expand own territory in the given direction.\"\"\"\n",
    "        dr, dc = self.DIRECTIONS[action]\n",
    "        new_cells = set()\n",
    "        \n",
    "        for r, c in list(self.own_cells):\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < self.GRID_SIZE and 0 <= nc < self.GRID_SIZE:\n",
    "                if self.grid[nr, nc] != 1.0:  # Not already own\n",
    "                    new_cells.add((nr, nc))\n",
    "        \n",
    "        # Expand to adjacent cells (not all, simulate troop cost)\n",
    "        expand_count = max(1, len(new_cells) // 3)\n",
    "        expand_cells = random.sample(list(new_cells), min(expand_count, len(new_cells)))\n",
    "        \n",
    "        for r, c in expand_cells:\n",
    "            old_val = self.grid[r, c]\n",
    "            self.grid[r, c] = 1.0\n",
    "            self.own_cells.add((r, c))\n",
    "            # Remove from enemy if it was enemy territory\n",
    "            if old_val == -1.0:\n",
    "                for enemy_set in self.enemy_cells:\n",
    "                    enemy_set.discard((r, c))\n",
    "    \n",
    "    def _enemy_expand(self, enemy_idx: int):\n",
    "        \"\"\"Simple enemy AI: expand toward own territory.\"\"\"\n",
    "        if not self.enemy_cells[enemy_idx] or not self.own_cells:\n",
    "            return\n",
    "        \n",
    "        # Find direction toward own territory\n",
    "        enemy_center = np.mean(list(self.enemy_cells[enemy_idx]), axis=0)\n",
    "        own_center = np.mean(list(self.own_cells), axis=0)\n",
    "        \n",
    "        diff = own_center - enemy_center\n",
    "        dr = int(np.sign(diff[0]))\n",
    "        dc = int(np.sign(diff[1]))\n",
    "        \n",
    "        # Expand one cell\n",
    "        for r, c in list(self.enemy_cells[enemy_idx]):\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < self.GRID_SIZE and 0 <= nc < self.GRID_SIZE:\n",
    "                if self.grid[nr, nc] != -1.0:\n",
    "                    old_val = self.grid[nr, nc]\n",
    "                    self.grid[nr, nc] = -1.0\n",
    "                    self.enemy_cells[enemy_idx].add((nr, nc))\n",
    "                    if old_val == 1.0:\n",
    "                        self.own_cells.discard((nr, nc))\n",
    "                    break\n",
    "    \n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"Convert grid to state vector (64 features).\"\"\"\n",
    "        total = self.GRID_SIZE * self.GRID_SIZE\n",
    "        own_count = len(self.own_cells)\n",
    "        enemy_count = sum(len(e) for e in self.enemy_cells)\n",
    "        neutral_count = total - own_count - enemy_count\n",
    "        \n",
    "        state = np.zeros(64, dtype=np.float32)\n",
    "        \n",
    "        # Territory percentages\n",
    "        state[0] = own_count / total\n",
    "        state[1] = enemy_count / total\n",
    "        state[2] = neutral_count / total\n",
    "        \n",
    "        # Normalized counts\n",
    "        state[3] = own_count / total\n",
    "        state[4] = enemy_count / total\n",
    "        state[5] = neutral_count / total\n",
    "        \n",
    "        # Border counts\n",
    "        own_borders = self._count_borders(self.own_cells)\n",
    "        state[6] = own_borders / 100.0\n",
    "        \n",
    "        # Game phase\n",
    "        own_pct = own_count / total\n",
    "        if own_pct < 0.05:\n",
    "            state[8] = 1.0  # early\n",
    "        elif own_pct < 0.25:\n",
    "            state[9] = 1.0  # mid\n",
    "        else:\n",
    "            state[10] = 1.0  # late\n",
    "        \n",
    "        # Compact grid (5x10 = 50 features, starting at index 14)\n",
    "        sample_rows = np.linspace(0, self.GRID_SIZE - 1, 5, dtype=int)\n",
    "        sample_cols = np.linspace(0, self.GRID_SIZE - 1, 10, dtype=int)\n",
    "        idx = 14\n",
    "        for r in sample_rows:\n",
    "            for c in sample_cols:\n",
    "                if idx < 64:\n",
    "                    state[idx] = self.grid[r, c]\n",
    "                    idx += 1\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _count_borders(self, cells: set) -> int:\n",
    "        \"\"\"Count cells that are on the border of a territory.\"\"\"\n",
    "        count = 0\n",
    "        for r, c in cells:\n",
    "            for dr, dc in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "                nr, nc = r+dr, c+dc\n",
    "                if (nr, nc) not in cells:\n",
    "                    count += 1\n",
    "                    break\n",
    "        return count\n",
    "    \n",
    "    def render(self, ax=None):\n",
    "        \"\"\"Visualize the current grid state.\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        \n",
    "        display = np.zeros((self.GRID_SIZE, self.GRID_SIZE, 3))\n",
    "        display[self.grid == 0] = [0.5, 0.5, 0.5]   # neutral = gray\n",
    "        display[self.grid == 1] = [0.0, 0.4, 1.0]   # own = blue\n",
    "        display[self.grid == -1] = [1.0, 0.2, 0.2]  # enemy = red\n",
    "        \n",
    "        ax.imshow(display)\n",
    "        ax.set_title(f'Step {self.step_count} | Own: {len(self.own_cells)}')\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "# Test environment\n",
    "env = TerritorialEnv()\n",
    "state = env.reset()\n",
    "print(f'State shape: {state.shape}')\n",
    "print(f'State sample: {state[:10]}')\n",
    "\n",
    "# Visualize initial state\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "env.render(axes[0])\n",
    "axes[0].set_title('Initial State')\n",
    "\n",
    "# Take some random steps\n",
    "for _ in range(20):\n",
    "    s, r, done, info = env.step(random.randint(1, 8))\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break\n",
    "env.render(axes[1])\n",
    "axes[1].set_title('After 20 Steps')\n",
    "\n",
    "for _ in range(50):\n",
    "    s, r, done, info = env.step(random.randint(1, 8))\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break\n",
    "env.render(axes[2])\n",
    "axes[2].set_title('After 70 Steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/env_visualization.png', dpi=100)\n",
    "plt.show()\n",
    "print('Environment visualization saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network for Territorial.io decision making.\n",
    "    Architecture matches brain_system.py for direct weight loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size: int = 64, action_size: int = 9):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "STATE_SIZE = 64\n",
    "ACTION_SIZE = 9\n",
    "GAMMA = 0.95\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 500\n",
    "\n",
    "# Initialize networks\n",
    "policy_net = DQNNetwork(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "target_net = DQNNetwork(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "total_params = sum(p.numel() for p in policy_net.parameters())\n",
    "print(f'DQN parameters: {total_params:,}')\n",
    "print(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = policy_net(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "def train_step() -> Optional[float]:\n",
    "    \"\"\"Perform one DQN training step (Double DQN).\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states_t = torch.FloatTensor(np.array(states)).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "    rewards_t = torch.FloatTensor(rewards).to(device)\n",
    "    next_states_t = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "    dones_t = torch.BoolTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    q_values = policy_net(states_t).gather(1, actions_t.unsqueeze(1))\n",
    "    \n",
    "    # Double DQN target\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states_t).argmax(dim=1)\n",
    "        next_q = target_net(next_states_t).gather(\n",
    "            1, next_actions.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "        next_q[dones_t] = 0.0\n",
    "        target_q = rewards_t + GAMMA * next_q\n",
    "    \n",
    "    loss = criterion(q_values.squeeze(1), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Training\n",
    "env = TerritorialEnv()\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_own_pcts = []\n",
    "losses = []\n",
    "\n",
    "best_avg_reward = float('-inf')\n",
    "best_model_path = f'{OUTPUT_DIR}/brain_model_best.pth'\n",
    "\n",
    "print(f'Training DQN for {EPISODES} episodes...')\n",
    "print('=' * 70)\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES + 1)):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    step = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        loss = train_step()\n",
    "        if loss is not None:\n",
    "            losses.append(loss)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step + 1)\n",
    "    episode_own_pcts.append(info.get('own_pct', 0))\n",
    "    \n",
    "    # Save best model\n",
    "    if episode >= 50:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "        if avg_reward > best_avg_reward:\n",
    "            best_avg_reward = avg_reward\n",
    "            torch.save({\n",
    "                'policy_net': policy_net.state_dict(),\n",
    "                'target_net': target_net.state_dict(),\n",
    "                'epsilon': epsilon,\n",
    "                'episode': episode,\n",
    "            }, best_model_path)\n",
    "    \n",
    "    # Periodic logging\n",
    "    if episode % 100 == 0:\n",
    "        avg_r = np.mean(episode_rewards[-100:])\n",
    "        avg_own = np.mean(episode_own_pcts[-100:])\n",
    "        avg_loss = np.mean(losses[-1000:]) if losses else 0\n",
    "        print(\n",
    "            f'Episode {episode:5d}/{EPISODES} | '\n",
    "            f'Avg Reward: {avg_r:7.2f} | '\n",
    "            f'Avg Own%: {avg_own:.1%} | '\n",
    "            f'ε: {epsilon:.3f} | '\n",
    "            f'Loss: {avg_loss:.4f} | '\n",
    "            f'Memory: {len(memory)}'\n",
    "        )\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'Training complete! Best avg reward: {best_avg_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, window=50):\n",
    "    \"\"\"Moving average smoothing.\"\"\"\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
    "axes[0, 0].plot(smooth(episode_rewards), color='blue', label='Smoothed')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "axes[0, 1].plot(episode_lengths, alpha=0.3, color='green', label='Raw')\n",
    "axes[0, 1].plot(smooth(episode_lengths), color='green', label='Smoothed')\n",
    "axes[0, 1].set_title('Episode Lengths')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Territory ownership\n",
    "axes[1, 0].plot(episode_own_pcts, alpha=0.3, color='orange', label='Raw')\n",
    "axes[1, 0].plot(smooth(episode_own_pcts), color='orange', label='Smoothed')\n",
    "axes[1, 0].set_title('Territory Ownership at Episode End')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Own Territory %')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Training loss\n",
    "if losses:\n",
    "    axes[1, 1].plot(losses, alpha=0.2, color='red', label='Raw')\n",
    "    axes[1, 1].plot(smooth(losses, 200), color='red', label='Smoothed')\n",
    "    axes[1, 1].set_title('Training Loss')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('MSE Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "plt.suptitle('DQN Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/brain_training_curves.png', dpi=100)\n",
    "plt.show()\n",
    "print('Training curves saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "policy_net.eval()\n",
    "\n",
    "print('Evaluating trained agent (10 episodes, greedy policy)...')\n",
    "eval_rewards = []\n",
    "eval_own_pcts = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for ep in range(10):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        # Greedy action (no exploration)\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action = policy_net(state_t).argmax(dim=1).item()\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(total_reward)\n",
    "    eval_own_pcts.append(info.get('own_pct', 0))\n",
    "    \n",
    "    # Visualize final state\n",
    "    row, col = ep // 5, ep % 5\n",
    "    env.render(axes[row, col])\n",
    "    axes[row, col].set_title(\n",
    "        f'Ep {ep+1}: R={total_reward:.1f} Own={info[\"own_pct\"]:.1%}',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.suptitle('Trained Agent Evaluation (10 Episodes)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/agent_evaluation.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nEvaluation Results:')\n",
    "print(f'  Avg Reward: {np.mean(eval_rewards):.2f} ± {np.std(eval_rewards):.2f}')\n",
    "print(f'  Avg Own Territory: {np.mean(eval_own_pcts):.1%} ± {np.std(eval_own_pcts):.1%}')\n",
    "print(f'  Best Episode: {max(eval_rewards):.2f} reward, {max(eval_own_pcts):.1%} territory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = f'{OUTPUT_DIR}/brain_model.pth'\n",
    "torch.save({\n",
    "    'policy_net': policy_net.state_dict(),\n",
    "    'target_net': target_net.state_dict(),\n",
    "    'epsilon': EPSILON_END,  # Start with low epsilon for deployment\n",
    "    'episode': EPISODES,\n",
    "    'state_size': STATE_SIZE,\n",
    "    'action_size': ACTION_SIZE,\n",
    "    'hyperparams': {\n",
    "        'gamma': GAMMA,\n",
    "        'lr': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "    }\n",
    "}, final_model_path)\n",
    "print(f'Final brain model saved to: {final_model_path}')\n",
    "\n",
    "# Verify\n",
    "test_model = DQNNetwork(STATE_SIZE, ACTION_SIZE)\n",
    "ckpt = torch.load(final_model_path, map_location='cpu')\n",
    "test_model.load_state_dict(ckpt['policy_net'])\n",
    "test_model.eval()\n",
    "\n",
    "dummy_state = torch.randn(1, STATE_SIZE)\n",
    "with torch.no_grad():\n",
    "    q_vals = test_model(dummy_state)\n",
    "print(f'Test Q-values shape: {q_vals.shape}')\n",
    "print(f'Test Q-values: {q_vals.numpy()}')\n",
    "print('Brain model verification passed!')\n",
    "\n",
    "# List output files\n",
    "print('\\nOutput files:')\n",
    "for f in sorted(Path(OUTPUT_DIR).iterdir()):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f'  {f.name}: {size_kb:.1f} KB')\n",
    "\n",
    "print('\\n✅ DONE! Download brain_model.pth')\n",
    "print('   Place it in territorial_bot/models/brain_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
